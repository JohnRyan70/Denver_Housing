

# Predicting the Denver Housing Market
### John Francis Ryan (jfryan70@gmail.com)

# Part Four: What Explains Neighborhood Housing values?
## Dealing with Missing Values

Now you shift gears and begin to explore variations in the housing market. Why do some neighborhoods have more expensive houses than others? 

Before going ahead, it is important to prepare the data. One problem with the data pertains to missing values. As seen previously, nine census tracts do not have a median housing value and thus you must remove them from the dataset prior to starting the analysis.


```{r}

#################################
#

setwd('C:/Users/John/Denver_Housing_Project/ACS_Data/Final_Data')

load("ACSData9_14.RData")

# Identify and remove missing values from analysis

table(is.na(ACSData9_14$MedHouseVal))

# Noticing ten missing observations of median housing value, and taking out 
test <- subset(ACSData9_14, is.na(MedHouseVal)) # So you can look at the 2 observations missing this variable
ACSData9_14nona <- subset(ACSData9_14, !is.na(MedHouseVal)) 
table(is.na(ACSData9_14nona$MedHouseVal)) #Now no NA's in median housing value
save(ACSData9_14, file = "ACSData9_14.RData")


```
## Narrowing and Splitting the Data

Given the desire to discover an algorithm that is a) fairly parsimonious, b) easy to explain and c) with good explanatory power, you search through the previous correlations and narrow it down to eleven factors. 

First, you include two of three household income groupings, one to represent upper-class household income (above $100K) and another for middle-class income ($35 to $100K). In this way, you include the most interesting groups while keeping the lower-class income group out as a control. You choose not to include more specific groups (e.g. percent $35 to $50K) both to reduce multicollinearity and provide a more general story about income groups. You are curious to explore the surprising alleged negative link between middle-class income and housing values. You also include median individual (as opposed to household) income to tap into non-family housing and non-homeowners in general.

Second, you include two of three basic education groups, one for high-school dropouts and another for college graduates (and above); high school-only graduates remain as a control group. You suspect a negative relationship between high school dropout percentage and housing values, and a positive one for college graduates and housing values. 

Third, you look at a few other family demographics. First, you include the percentage of households as married-couple families, expecting a positive relationship. Second, you include average household family size to explore whether it affects housing values. Third, you investigate whether 'older' neighborhoods - here, median age in the neighborhood - tend to affect housing values.

Fourth, you include a few indicators for the overall neighborhood economy. First, is it true that vacancies in the neighborhood depress its overall economic value? You include the percentage of building vacancies to tap into this. Second, since you believe neighborhood poverty affects potential buyers you include the percentage of people living below 150% of the poverty level (as defined by the Census). You also include the average number of hours worked.

Last, you decide to investigate further whether school quality still has an impact on housing values.

With these in mind, you subset the initial dataset of 120+ variables to only those of interest. 

Of course, one of the problems in any statistical analysis is overfitting the data. If your algorithm is too complicated, it is practically useless in another context. A fun example for presidential elections is this [XKCD comic](https://xkcd.com/1122).

```{r message=FALSE, warning=FALSE}

# Now, to test models using regression

library(corrplot)
library(foreign)
library(glmnet)
library(broom)
library(foreign)
library(MASS)
library(car)
library(ggplot2)

ACSData9_14nona.mod <-ACSData9_14nona[c("MedHouseVal",
                  "HHInc_Own_PercUnder35K",
                  "HHInc_Own_Perc35Kto100K",
                  "HHInc_Own_PercOver100K",
                  "IndInc_Median",
                  "PercNotHSGrad",
                  "PercBAorMore",
                  "HHs_Perc_MarriedCplFam",
                  "HHs_AvgFamilySize",
                  "MedianAge",
                  "PercUnder150PercPov",
                  "Housing_Perc_Vacant",
                  "MeanHrsWkd",
                  "SchlDistRnkLYr")]


```

In order to avoid potential overfitting, you then split up the data into two datasets - one to design (or 'train') a statistical model, the other, to test the effectiveness of said model on new data. If the training model performs poorly on the test dataset, it is time to go back to the drawing board!

```{r}
# Create a training/test split of 70% train, 30% test

set.seed(1234)
ind <-sample(2,nrow(ACSData9_14nona.mod), replace=TRUE, prob=c(0.7, 0.3))
ACSData9_14nona.mod.train <-ACSData9_14nona.mod[ind==1,]
ACSData9_14nona.mod.test <-ACSData9_14nona.mod[ind==2,]

table(is.na(ACSData9_14nona.mod.train$MedHouseVal)) # 400 observations
table(is.na(ACSData9_14nona.mod.test$MedHouseVal)) # 178 observations

save(ACSData9_14nona, file = "ACSData9_14nona.RData")
save(ACSData9_14nona.mod, file = "ACSData9_14nona.mod.RData")
save(ACSData9_14nona.mod.train, file = "ACSData9_14nona.mod.training.RData")
save(ACSData9_14nona.mod.test, file = "ACSData9_14nona.mod.test.RData")

```

## Analysis: Regression

Now that you have narrowed down and split the data, you can begin to analyze the data. You begin with linear regression for a variety of reasons: it is simple, familiar, and easy to interpret the results. 

### Initial Results
So, which ones will remain influential? Let's see! 

```{r}


# Modeling using OLS first

model1 <- lm(MedHouseVal ~ 
                  HHInc_Own_Perc35Kto100K +
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercNotHSGrad +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  MeanHrsWkd +
                  SchlDistRnkLYr,
                  data = ACSData9_14nona.mod.train)

# make predictions and summarize accuracy
predictions1 <- predict(model1, ACSData9_14nona.mod.train)

model1.res = resid(model1)
summary(model1)
AIC(model1)
BIC(model1)
summary(model1.res)
#ggplot(model1.res)
sm <-summary(model1)
sum(sm$residuals^2)
sqrt(mean(sm$residuals^2))

vif(model1)

```


You notice a few things from the results above. First, the explanatory power of the model is robust, with an adjusted R-squared of 0.76. (Keep in mind the residual standard error of 65,763 for later.) You also notice more than one coefficient is in a direction opposite to what you expected. 

For now, you are concerned about multicollinearity: the variance inflation factor (VIF) scores are high in some cases. 

You decide to run a few models with all variables but one, to see the potential impact of related variables.

```{r}

model2 <- lm(MedHouseVal ~ 
                  HHInc_Own_Perc35Kto100K +
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  MeanHrsWkd +
                  SchlDistRnkLYr,
                  data = ACSData9_14nona.mod.train)

# make predictions and summarize accuracy
predictions2 <- predict(model2, ACSData9_14nona.mod.train)

model2.res = resid(model2)
summary(model2)
AIC(model2)
BIC(model2)
summary(model2.res)
vif(model2)


model3 <- lm(MedHouseVal ~ 
                  HHInc_Own_Perc35Kto100K +
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercNotHSGrad +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  SchlDistRnkLYr,
                  data = ACSData9_14nona.mod.train)

# make predictions and summarize accuracy
predictions3 <- predict(model3, ACSData9_14nona.mod.train)

model3.res = resid(model3)
summary(model3)
AIC(model3)
BIC(model3)
summary(model3.res)
#ggplot(model1.res)
vif(model3)


model4 <- lm(MedHouseVal ~ 
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercNotHSGrad +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  MeanHrsWkd +
                  SchlDistRnkLYr,
                  data = ACSData9_14nona.mod.train)


# make predictions and summarize accuracy
predictions4 <- predict(model4, ACSData9_14nona.mod.train)

model4.res = resid(model4)
summary(model4)
AIC(model4)
BIC(model4)
summary(model4.res)
#ggplot(model1.res)

vif(model4)

library(corrgram)
corrgram(ACSData9_14nona.mod.train, order=NULL, 
         lower.panel=panel.shade,
         upper.panel=NULL,
         text.panel=panel.txt,
         main="Correlogram of data")


```

### Multicollinearity

You notice from the correlogram above that there is some collinearity between the variables. This should come as little surprise among some of them. 

You try calculating the variance inflation factor (VIF) resulting from each model. Some scores are pretty high - from 7.48 to 8.54 - but not as high as a benchmark value of 10. However, you still suspect collinearity within the model for a few reasons. First, your model includes a variety of related socioeconomic factors (i.e. income, education) that are highly correlated. So, even though INDIVIDUAL income is conceptually different from HOUSEHOLD income, they still correlate highly (r = .73 to .80). You also find a high correlation between income and education (minimum of 0.69). Further, poverty correlates highly with both income and education (r > .73). 

Just to be sure, you run a few slightly-similar models but dropping one variable. The first model, dropping the high-school dropouts factor, does not affect the model greatly but does flip the sign of the average family size factor. The second model, dropping only the average hours worked factor, does change the direction of the intercept. The third, dropping the percentage of household owners $35 to $100K, finds little movement among the remaining variables. More importantly, however, you notice the high VIF scores among some variables within these models. How can you have confidence in your modeling, with such difficulties?


## Multicollinearity and LASSO Regression 
Given the rather high level of collinearity among variables, you decide to use LASSO regression. With this method, you can run the original model while introducing a 'penalty' for collinearity among variables. Taken from a different perspective, it uses a "tuning parameter" to adjust the model. A good resource is [the first LASSO paper](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf).

Some people might choose to use the one-standard-error lambda, which "err[s] on the side of parsimony" (Hastie T, Tibshirani R, Friedman J, *The Elements of Statistical Learning: Prediction, Inference and Data Mining,* 2nd edition (2009)). However, because you are seeking a model straddling both parsimony and predictive power, you opt for the lambda that provides the lowest mean squared error. 

Let us see what happens!

```{r message=FALSE, warning=FALSE}

############################
#
#
#  Try LASSO regression to reduce multicollinearity

# Preparing matrices for modeling
# We include a -1 at the end of x.train and x.test, to drop a duplicate intercept term
set.seed(1111)
x.train=model.matrix(MedHouseVal ~ 
                  HHInc_Own_Perc35Kto100K +
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercNotHSGrad +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  MeanHrsWkd +
                  SchlDistRnkLYr
                  - 1,
                  data = ACSData9_14nona.mod.train)


y.train=ACSData9_14nona.mod.train$MedHouseVal

x.test=model.matrix(MedHouseVal ~ 
                  HHInc_Own_Perc35Kto100K +
                  HHInc_Own_PercOver100K +
                  IndInc_Median +
                  PercNotHSGrad +
                  PercBAorMore +
                  HHs_Perc_MarriedCplFam +
                  HHs_AvgFamilySize +
                  MedianAge +
                  PercUnder150PercPov +
                  Housing_Perc_Vacant +
                  MeanHrsWkd +
                  SchlDistRnkLYr
                  - 1,
                  data = ACSData9_14nona.mod.test)

# Performing lasso, with cross-validation, ten folds (default; can choose more)
set.seed(1111)
fit.lasso=cv.glmnet(x.train,y.train,alpha=1)

# Discovering the best lambda, using minimum (not one-standard-error)

```

Here are the coefficients for the 'one-standard-error' rule:

```{r}

coef(fit.lasso, s = "lambda.1se")

```

Here are the coefficients for the minimum lambda rule:

```{r message=FALSE, warning=FALSE}

coef(fit.lasso, s = "lambda.min")
```

You plot the RMSE for models using various tuning parameters, with dashed lines signaling the 'minimum' error lambda (on the left) and 'one-standard-error' lambda (on the right). In other words, it highlights the trade-off between predictive power and parsimony.

```{r message=FALSE, warning=FALSE}

plot(fit.lasso,xvar="lambda",label=TRUE)

```

For the sake of interest, you also plot the change in coefficients as the tuning parameter changes.


```{r}
plot(fit.lasso$glmnet.fit, "lambda", label=TRUE)


```

What do you see so far? Probably the largest thing naturally, is the change in the number of coefficients. The model with the one-standard-error has fewer ones, in particular. Since you are searching for both parsimony and low error, you stick with the less-parsimonious model.

## Comparing OLS and LASSO Results

How well does this perform against OLS, however? You calculate a common metric: the root mean-squared error (RMSE). This gives an indication of the model's performance: how much does it make prediction errors?


```{r}

bestlamb.lasso <- fit.lasso$lambda.min
bestlamb.lasso
pred.lasso <-predict(fit.lasso, s=bestlamb.lasso, newx=x.train)
lasso.err <-mean((ACSData9_14nona.mod.train$MedHouseVal-pred.lasso)^2)
sqrt(lasso.err)

# Check coefficients and RMSE with test data
reg.lasso <-cv.glmnet(x.train,y.train, alpha=1)
pred.lasso2 <-predict(reg.lasso, s=bestlamb.lasso, newx=x.test)
lasso.err2 <-mean((ACSData9_14nona.mod.test$MedHouseVal-pred.lasso2)^2)
coef.lasso2 <-predict(reg.lasso, type="coefficients", s=bestlamb.lasso)
coef.lasso2
sqrt(lasso.err2)


```

Upon reviewing the LASSO model, a few things come to mind. First, you notice the RMSE is very close to OLS (65,880 to 65,736, respectively). This suggests the LASSO model is competitive with OLS.

Second, you notice its performance comes with a slightly more parsimonious model, as the LASSO model dropped the middle-income and family size variables (both of which were insignificant in the OLS model). As expected with the LASSO procedure, many coefficients in the model decrease in size but do not change their direction. Third, as seen below, the coefficients for the LASSO model (using the test data) are quite similar in size and the RMSE although the RMSE is a bit larger (70,767). This suggests a reasonably good model.

## Results

A few things take shape as well. You notice the importance of household income, such as the percentage of households with $100K: everything else equal, the difference between a neighborhood with 20 percent vs. 60 percent is almost $176,000. That in itself is a jump, considering the middle range of housing is from $180,000 to $330,000. You also notice how an area's median personal income can have an effect on housing values, with every $1.00 increase in income increases housing by $1.51. 

You find that family and personal demographics have a mixed effect (at best) upon housing values. The percent of married couples does appear positive but very small, with a $1,200 difference occurring with a 30 percentage point difference between neighborhoods. This suggests a very weak, positive effect. The average family size appears to wash out in the LASSO model. The model also finds a negative relationship with the average number of hours worked: for every one more hour worked, the neighborhood value decreases by $6,587. Last, the median neighborhood population age appears to have a positive effect: one can find a $45,915 housing value difference between two neighborhoods with a 15 year age difference. Perhaps this is due, in part, to an unspoken minimum age needed to purchase a house: how many 18-year olds can normally qualify for a house loan? 

You also see the importance of college education in a neighborhood. The difference between a neighborhood with a 20 percent versus 60 percent rate of college graduates can be just above $97,000! However, you are surprised at the (apparently) negative effect of dropout rates. Is it also true that high school dropout rates also increase median housing? 

A few other surprises occur in our model as well. The results suggest housing vacancies actually INCREASE the median house value; the same appears with poverty levels. While the OLS results suggest vacancies are not significant, they do suggest poverty levels are. Is it true that a 25 percentage point increase in poverty levels depresses housing by $33,600? Something just seems untrue about this, and yet the overall magnitude is not so strong as to cause more than some concern. 

Another surprising finding pertains to the influence of primary and secondary school education. The model coefficient (-31,569) suggests that a public school district's rating depresses housing. However, the OLS model found this marginally significant (0.05 < p < 0.10) and the effect somewhat small. Second, a public school district covers many neighborhoods, each with variations in housing; thus, a more refined indicator (e.g. nearest high school) might be in order.

# Conclusion

What can you take away from this? Perhaps the largest is the importance of income and education, with household and individual income affecting housing values directly. Thus, if you want to know where to find top-level housing, it helps to know not simply the wealthy areas but also the areas with the greatest concentration of highly-educated people.

Something else you consider is how to take some modeling with a grain of salt. You noticed school district rankings having a negative impact, but given the marginal significance level and your own thoughts - using a multi-neighborhood variable to explain neighborhood-level fluctuations - you consider the findings inconclusive for now. You have similar thoughts about the vacancy factor, since the OLS models find it to be marginal. 

A few things you consider exploring. Why might the results suggest a neighborhood's high school dropout rate to increase housing values, and the same for poverty rates and housing? That is something to explore later. 